import os, json, traceback
from functools import lru_cache

from django.shortcuts import render
from django.http import JsonResponse
from django.db.models import F, Value, FloatField, ExpressionWrapper
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from .models import MyDocument
from pgvector.django import CosineDistance

from langchain_huggingface import HuggingFaceEmbeddings
from langchain.prompts import PromptTemplate
from langchain_core.runnables import RunnableMap
from django.views.decorators.csrf import csrf_exempt   # ‚úÖ ‡∏õ‡∏¥‡∏î CSRF ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß

# ---------- ENV ----------
load_dotenv()
HF_TOKEN = os.getenv("HUGGING_FACE_HUB_API_TOKEN")
if not HF_TOKEN:
    raise RuntimeError("HUGGING_FACE_HUB_API_TOKEN ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏ô .env")
os.environ["HUGGING_FACE_HUB_API_TOKEN"] = HF_TOKEN

# ---------- GLOBALS ----------
EMBEDDER = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

@lru_cache(maxsize=1)
def get_llm():
    return ChatGoogleGenerativeAI(
        model="gemini-1.5-flash",   # ‚úÖ ‡πÉ‡∏ä‡πâ flash (‡πÑ‡∏ß‡∏Å‡∏ß‡πà‡∏≤ pro)
        google_api_key=os.getenv("GEMINI_API_KEY"),
        temperature=0.2,
        max_output_tokens=512,
    )

# ---------- PROMPTS ----------
RAG_PROMPT = """
‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÇ‡∏î‡∏¢‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å Context ‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
‡∏ñ‡πâ‡∏≤ Context ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏ß‡πà‡∏≤: "‡∏â‡∏±‡∏ô‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ô‡∏µ‡πâ"

Context:
{context}

‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:
{question}

‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 2‚Äì3 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ
"""

OPEN_PROMPT = """
You are a helpful assistant. Explain clearly and concisely.

Question:
{question}

Answer in Thai, 2‚Äì3 sentences, give a short example if useful.
"""

GREETINGS = {"hi", "hello", "hey", "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ", "‡∏´‡∏ß‡∏±‡∏î‡∏î‡∏µ", "‡πÄ‡∏Æ‡∏•‡πÇ‡∏•‡πà"}

def _fast_greeting_reply(q: str) -> str | None:
    qn = (q or "").strip().lower()
    if qn in GREETINGS or any(qn.startswith(g) for g in GREETINGS):
        return "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡πà‡∏∞ üòä ‡∏â‡∏±‡∏ô‡∏ä‡πà‡∏ß‡∏¢‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏î‡πâ ‡∏•‡∏≠‡∏á‡∏ñ‡∏≤‡∏°‡∏ß‡πà‡∏≤ ‚ÄúPython ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£‚Äù ‡∏´‡∏£‡∏∑‡∏≠ ‚Äú‡∏°‡∏µ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏≠‡∏∞‡πÑ‡∏£‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ö‡πâ‡∏≤‡∏á?‚Äù"
    return None

def _force_to_text(x) -> str:
    try:
        if x is None:
            return ""
        if hasattr(x, "content"):
            return str(x.content)
        if isinstance(x, str):
            return x
        if isinstance(x, dict):
            if "generated_text" in x:
                return str(x["generated_text"])
            if "text" in x:
                return str(x["text"])
            return json.dumps(x, ensure_ascii=False)
        if isinstance(x, list) and x:
            return _force_to_text(x[0])
        return str(x)
    except Exception:
        return ""

# ---------- MAIN VIEW ----------
@csrf_exempt   # ‚úÖ ‡∏õ‡∏¥‡∏î CSRF (‡πÉ‡∏ä‡πâ‡∏ï‡∏≠‡∏ô dev/test)
def rag_query_view(request):
    if request.method != "POST":
        return render(request, "core/rag_query.html")

    try:
        user_query = (request.POST.get("query") or "").strip()
        if not user_query and request.body:
            try:
                data = json.loads(request.body.decode("utf-8"))
                user_query = (data.get("query") or "").strip()
            except Exception:
                pass

        if not user_query:
            return JsonResponse({"answer": "‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÉ‡∏™‡πà‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°"}, status=400)

        # Quick reply
        quick = _fast_greeting_reply(user_query)
        if quick:
            return JsonResponse({"answer": quick})

        llm = get_llm()

        # ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏™‡∏±‡πâ‡∏ô ‡πÜ ‚Üí ‡∏™‡πà‡∏á‡πÑ‡∏õ Open
        if len(user_query) <= 2:
            prompt = PromptTemplate.from_template(OPEN_PROMPT)
            chain = RunnableMap({"question": lambda _: user_query}) | prompt | llm
            raw = chain.invoke({})
            return JsonResponse({"answer": _force_to_text(raw).strip() or "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÑ‡∏î‡πâ"})

        # Embed + ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
        qvec = EMBEDDER.embed_query(user_query)
        qs = (
            MyDocument.objects
            .annotate(distance=CosineDistance("embedding", qvec))
            .annotate(similarity=ExpressionWrapper(Value(1.0) - F("distance"), output_field=FloatField()))
            .order_by("-similarity")
        )
        THRESHOLD = 0.28
        hits = list(qs.filter(similarity__gte=THRESHOLD)[:5])

        if hits:
            # --- RAG mode ---
            context = "\n".join(d.text for d in hits)
            best_sim = float(getattr(hits[0], "similarity", 0.0))
            print(f"üîé MODE=RAG | hits={len(hits)} | best_sim={best_sim:.3f}")

            prompt = PromptTemplate.from_template(RAG_PROMPT)
            chain = RunnableMap({"context": lambda _: context, "question": lambda _: user_query}) | prompt | llm
            raw = chain.invoke({})
            answer = _force_to_text(raw).strip()
        else:
            # --- Open mode ---
            print(f"üîé MODE=OPEN | no hits")
            prompt = PromptTemplate.from_template(OPEN_PROMPT)
            chain = RunnableMap({"question": lambda _: user_query}) | prompt | llm
            raw = chain.invoke({})
            answer = _force_to_text(raw).strip()

        if not answer:
            answer = "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÑ‡∏î‡πâ"

        return JsonResponse({"answer": answer})

    except Exception as e:
        print("‚ùå VIEW ERROR:", repr(e))
        traceback.print_exc()
        return JsonResponse({"answer": f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}"}, status=500)


def home_view(request):
    return render(request, "core/rag_query.html")
